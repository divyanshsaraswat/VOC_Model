{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff93944",
   "metadata": {},
   "source": [
    "# Gas Concentration Regressor Training with Model Selection\n",
    "\n",
    "This notebook implements **Stage 2** of the gas classification project with an **AutoML-style model selection**.\n",
    "\n",
    "## Objective\n",
    "Data-driven selection of the best regression algorithms for each gas type to predict concentration.\n",
    "\n",
    "## Methodology\n",
    "1. **Data Loading**: Load sensor dataset.\n",
    "2. **Preprocessing**:\n",
    "   - Filter per gas.\n",
    "   - Target: Log1p transformation.\n",
    "3. **Model Selection**:\n",
    "   Compare the following algorithms:\n",
    "   - **HistGradientBoostingRegressor (HGBR)**: Fast, gradient boosting, handles NaNs.\n",
    "   - **RandomForestRegressor (RF)**: Robust ensemble method, reduced overfitting.\n",
    "   - **ExtraTreesRegressor (ET)**: Extremely randomized trees, often faster/better than RF.\n",
    "   - **Ridge Regression**: Linear baseline with regularization (requires scaling).\n",
    "4. **Evaluation**:\n",
    "   - Split Data (80/20).\n",
    "   - Train each candidate.\n",
    "   - Select best based on **RMSE** (Root Mean Squared Error).\n",
    "5. **Persistence**: Save best models to `models/regressors/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f4f22",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Add project root to path if running interactively\n",
    "current_dir = Path(os.getcwd())\n",
    "if str(current_dir) not in sys.path:\n",
    "    sys.path.append(str(current_dir))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "from src.utils import REGRESSORS_DIR, logger\n",
    "from src.data_loader import load_data, preprocess_for_regression\n",
    "from src.classifier import VOC_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badadd59",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35153a7b",
   "metadata": {},
   "source": [
    "## Candidate Definitions\n",
    "We define a dictionary of model pipelines.\n",
    "Note: Tree-based models (RF, ET, HGBR) generally don't need scaling, but Ridge does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b9a36",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def get_candidates():\n",
    "    return {\n",
    "        \"HGBR\": HistGradientBoostingRegressor(random_state=42),\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42),\n",
    "        \"ExtraTrees\": ExtraTreesRegressor(n_estimators=100, n_jobs=-1, random_state=42),\n",
    "        \"Ridge\": Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"model\", Ridge(alpha=1.0))\n",
    "        ])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab37403",
   "metadata": {},
   "source": [
    "## Training & Selection Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebee61",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "results_summary = []\n",
    "\n",
    "for gas_id, gas_name in VOC_LABELS.items():\n",
    "    logger.info(f\"\\n{'='*30}\\nProcessing {gas_name}\\n{'='*30}\")\n",
    "    \n",
    "    # 1. Data Prep\n",
    "    X, y_log = preprocess_for_regression(df, gas_name)\n",
    "    \n",
    "    if X is None or len(X) < 50:\n",
    "        logger.warning(f\"Skipping {gas_name}: Insufficient data.\")\n",
    "        continue\n",
    "        \n",
    "    X_train, X_test, y_train_log, y_test_log = train_test_split(\n",
    "        X, y_log, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Transform targets for metric calculation\n",
    "    y_test_orig = np.expm1(y_test_log)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_model_name = None\n",
    "    best_model_obj = None\n",
    "    best_metrics = {}\n",
    "    \n",
    "    candidates = get_candidates()\n",
    "    \n",
    "    for name, model in candidates.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train_log)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_log = model.predict(X_test)\n",
    "        \n",
    "        # Safe inverse transform\n",
    "        # Clip log predictions to avoid overflow (exp(100) is huge enough)\n",
    "        y_pred_log = np.clip(y_pred_log, -2, 100)\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        y_pred = np.maximum(y_pred, 0) # Clip negative predictions\n",
    "        \n",
    "        # Metrics\n",
    "        mae = mean_absolute_error(y_test_orig, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred))\n",
    "        r2 = r2_score(y_test_orig, y_pred)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        logger.info(f\"[{name}] RMSE: {rmse:.3f} | MAE: {mae:.3f} | R2: {r2:.3f} | Time: {elapsed:.2f}s\")\n",
    "        \n",
    "        # Selection Logic (Minimize RMSE)\n",
    "        if rmse < best_loss:\n",
    "            best_loss = rmse\n",
    "            best_model_name = name\n",
    "            best_model_obj = model\n",
    "            best_metrics = {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "            \n",
    "    logger.info(f\"ðŸ† Winner for {gas_name}: {best_model_name} (RMSE: {best_loss:.3f})\")\n",
    "    \n",
    "    # Save Winner\n",
    "    save_path = REGRESSORS_DIR / f\"{gas_name.lower()}_reg.pkl\"\n",
    "    joblib.dump(best_model_obj, save_path)\n",
    "    \n",
    "    results_summary.append({\n",
    "        'Gas': gas_name,\n",
    "        'Best Model': best_model_name,\n",
    "        **best_metrics\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ecd49",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Final Report\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "print(\"\\n=== Model Selection Summary ===\")\n",
    "print(summary_df)\n",
    "summary_df.to_csv(\"regressor_selection_summary.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
